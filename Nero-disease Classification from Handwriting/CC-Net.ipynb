{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## CC- Net Reimplementation\n","\n","\n","- Reimplement from section 4"],"metadata":{"id":"q4DV_Wi7joFh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oc2X-SjfjdNa"},"outputs":[],"source":["import kagglehub\n","import os\n","import torch\n","from torchvision.datasets import ImageFolder\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, random_split, ConcatDataset\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from google.colab import drive\n","import random\n"]},{"cell_type":"code","source":["drive.mount('/content/drive')\n","print(os.listdir('/content/drive/MyDrive'))\n","# os.chdir('/content/drive/MyDrive/Class Materials/Fall 25/STOR566/STOR566Project') # Please update this line with the correct path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BbNxN4Z_BekZ","executionInfo":{"status":"ok","timestamp":1763587144873,"user_tz":300,"elapsed":685,"user":{"displayName":"Alyssa Zhao","userId":"01502609981018653962"}},"outputId":"34a2f431-14c9-46d6-cb25-e112e518d187"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","['College Application', 'Stats', 'CompSci', 'Class Materials', 'Colab Notebooks', 'Miscellaneous.gdoc', '剧本杀.gdoc']\n"]}]},{"cell_type":"code","source":["seed = 2025\n","torch.manual_seed(seed), np.random.seed(seed), random.seed(seed)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PW5oZBMOO2f1","executionInfo":{"status":"ok","timestamp":1763586997788,"user_tz":300,"elapsed":13,"user":{"displayName":"Alyssa Zhao","userId":"01502609981018653962"}},"outputId":"634ea5e9-8a72-4433-af2e-7b1bd6b49b46"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<torch._C.Generator at 0x7c322b788f50>, None, None)"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["## Dataset reformat and dataloader"],"metadata":{"id":"ZUKahzTMKswL"}},{"cell_type":"code","source":["def handpd_loaders(path, shuffle_test = False):\n","\n","    # reformat\n","    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                     std=[0.225, 0.225, 0.225])\n","    # paper resized to 227 * 227\n","    transform = transforms.Compose([\n","        transforms.Resize((227, 227)),\n","        transforms.ToTensor(),\n","        normalize\n","    ])\n","\n","    full_dataset = ImageFolder(path, transform = transform)\n","\n","    # 80-20 split\n","    train_size = int(0.8 * len(full_dataset))\n","    test_size = len(full_dataset) - train_size\n","    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n","\n","    # loaders\n","    train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True, pin_memory = True)\n","    test_loader  = DataLoader(test_dataset, batch_size = 32, shuffle = shuffle_test, pin_memory = True)\n","\n","    # properties\n","    print(f\"Total samples: {len(full_dataset)}\")\n","    print(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n","    print(f\"Classes: {full_dataset.classes}\")\n","\n","    return train_loader, test_loader, full_dataset\n"],"metadata":{"id":"Nl9OYY_SIb7F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_path = '/content/drive/MyDrive/Class Materials/Fall 25/STOR566/STOR566Project/Parkinsons'\n","train_loader, test_loader, full_dataset = handpd_loaders(base_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ymj1hBWrId2r","executionInfo":{"status":"ok","timestamp":1763587012957,"user_tz":300,"elapsed":10230,"user":{"displayName":"Alyssa Zhao","userId":"01502609981018653962"}},"outputId":"09571b69-0847-4dfb-9794-03904c75abdf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total samples: 3264\n","Train samples: 2611, Test samples: 653\n","Classes: ['Healthy', 'Parkinson']\n"]}]},{"cell_type":"code","source":["from collections import Counter"],"metadata":{"id":"1LF1mPbGekIU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["counts = Counter(full_dataset.targets)\n","print(\"Class index mapping:\", full_dataset.class_to_idx)\n","print(\"Total distribution:\", counts)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MCbVaEVKezwL","executionInfo":{"status":"ok","timestamp":1763587022104,"user_tz":300,"elapsed":15,"user":{"displayName":"Alyssa Zhao","userId":"01502609981018653962"}},"outputId":"b02cd9e3-6608-473f-b54e-ba7960eed268"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Class index mapping: {'Healthy': 0, 'Parkinson': 1}\n","Total distribution: Counter({0: 1632, 1: 1632})\n"]}]},{"cell_type":"code","source":["images, labels = next(iter(train_loader))\n","dataset = ImageFolder(base_path)\n","# labels -- 0(healthy) or 1(Parkinson)\n","print(images.shape, labels.shape)\n","print(dataset.classes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UqefZ1bJNVpC","executionInfo":{"status":"ok","timestamp":1763587055769,"user_tz":300,"elapsed":32198,"user":{"displayName":"Alyssa Zhao","userId":"01502609981018653962"}},"outputId":"71a3419e-fd14-4148-97e4-262ccabcac87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 3, 227, 227]) torch.Size([32])\n","['Healthy', 'Parkinson']\n"]}]},{"cell_type":"markdown","source":["## Visualize one example"],"metadata":{"id":"88Wj8DiLFpsP"}},{"cell_type":"code","source":["# Pick the first image\n","img = images[31]\n","lbl = labels[31]\n","\n","# Undo normalization for display\n","mean = np.array([0.485, 0.456, 0.406])\n","std = np.array([0.225, 0.225, 0.225])\n","#transform\n","img = img.numpy().transpose((1, 2, 0))\n","img = std * img + mean\n","img = np.clip(img, 0, 1)\n","# Access the original dataset classes\n","classes = train_loader.dataset.dataset.classes\n","\n","# Plot the image\n","plt.imshow(img)\n","plt.title(f\"Label: {classes[lbl]}\")\n","plt.axis('off')\n","plt.show()"],"metadata":{"id":"zpyenrNdBChn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Preprocessing not possible here becasue the dataset is combined and not applicable for the properties and only applied to spiral images"],"metadata":{"id":"G-DLIUntQtNZ"}},{"cell_type":"markdown","source":["## Model Implementation"],"metadata":{"id":"aoZy8acrQ8ZF"}},{"cell_type":"code","source":["class handpd_ccnet(nn.Module):\n","  def __init__(self, num_classes = 2):\n","    super(handpd_ccnet, self).__init__()\n","\n","    self.classifier = nn.Sequential(\n","        # module A\n","        # layer 1, 227 -> 56\n","        nn.Conv2d(in_channels = 3, out_channels = 48, kernel_size = 11, stride = 4, padding = 2),\n","        nn.ReLU(),\n","        #layer 2 56 -> 28\n","        nn.Conv2d(in_channels = 48, out_channels = 128, kernel_size = 5, stride = 2, padding = 2),\n","        nn.ReLU(),\n","        #Layer 3 28 -> 9\n","        nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 1),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size = 3, stride = 3),\n","\n","        #module B\n","        #layer 1 9 -> 9\n","        nn.Conv2d(in_channels = 128, out_channels = 192, kernel_size = 3, stride = 1, padding = 1),\n","        nn.ReLU(),\n","        #layer 2 9 -> 9\n","        nn.Conv2d(in_channels = 192, out_channels = 192, kernel_size = 3, stride = 1, padding = 1),\n","        nn.ReLU(),\n","        # layer 3 9 -> 5 -> 1\n","        nn.Conv2d(in_channels = 192, out_channels = 128, kernel_size = 3, stride = 2, padding = 1),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size = 3, stride = 3),\n","\n","        # optional since 1*1\n","        nn.Flatten(),\n","        # FC\n","        nn.Linear(in_features = 128 * 1 * 1, out_features = num_classes)\n","        )\n","\n","  def forward(self, x):\n","    return self.classifier(x)\n","\n"],"metadata":{"id":"KLTgjelGQ-Zc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"ugLOxC09WgzN"}},{"cell_type":"code","source":["from tqdm import tqdm"],"metadata":{"id":"v_4bY6ZvXTqF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"4D8QvmFPW2Hr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cc_net_model = handpd_ccnet().to(device)\n","print(cc_net_model)"],"metadata":{"id":"ZoEmsbYdWrjO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# evaluation matrix\n","#Accuracy, precision, recall, F1-score\n","learning_rate = 0.0001\n","# originally 2000\n","num_of_epoch = 50\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(cc_net_model.parameters(), lr = learning_rate)\n","\n","total_loss = []\n","for epoch in tqdm(range(num_of_epoch)):\n","    cc_net_model.train()\n","    running_loss = 0.0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = cc_net_model(images)\n","        loss = loss_fn(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    avg_loss = running_loss / len(train_loader)\n","    total_loss.append(avg_loss)\n","\n","    print(f\"Epoch [{epoch+1}/{num_of_epoch}], Loss: {avg_loss:.4f}\")\n","\n"],"metadata":{"id":"cxwZhl06FFRm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loss Tracking Plot\n","x = np.arange(num_of_epoch)\n","y = total_loss\n","plt.plot(x, y)\n","plt.title(\"Avergae Epoch Loss\")\n","plt.xlabel(\"number of epochs\")\n","plt.ylabel(\"loss\")\n","plt.show()"],"metadata":{"id":"WJOiyVMmYlaT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score"],"metadata":{"id":"p2UPU2P8pHUo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Testing Accuracy\n","cc_net_model.eval()\n","\n","# Collect data for visuals\n","correct = 0\n","total = 0\n","total_predictions = []\n","total_labels = []\n","probs = []\n","\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        outputs = cc_net_model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        softmax_probs = F.softmax(outputs, dim=1)[:,1]\n","\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","        total_predictions.extend(predicted.cpu().numpy())\n","        total_labels.extend(labels.cpu().numpy())\n","        probs.extend(softmax_probs.cpu().numpy())\n","\n","accuracy = correct / total\n","print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n"],"metadata":{"id":"1-RKF1zHZCHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# performance measurmenet\n","y_true = np.array(total_labels)\n","y_pred = np.array(total_predictions)\n","precision = precision_score(y_true, y_pred, average='binary')\n","recall = recall_score(y_true, y_pred, average='binary')\n","f1 = f1_score(y_true, y_pred, average='binary')\n","\n","\n","auc = roc_auc_score(y_true, probs)\n","\n","print(\"Evaluation Metrics:\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall:    {recall:.4f}\")\n","print(f\"F1-score:  {f1:.4f}\")\n","print(f\"AUC:       {auc:.4f}\")\n"],"metadata":{"id":"9d8CvKdTpdIj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def ccnet_visuals(image, title = None):\n","    image = image.numpy().transpose((1, 2, 0))\n","    image = image * np.array([0.225, 0.225, 0.225]) + np.array([0.485, 0.456, 0.406])\n","    image = np.clip(image, 0, 1)\n","    plt.imshow(image)\n","    if title:\n","        plt.title(title)\n","    plt.axis('off')\n","\n","# take first batch as examples\n","dataiter = iter(test_loader)\n","images, labels = next(dataiter)\n","images, labels = images.to(device), labels.to(device)\n","outputs = cc_net_model(images)\n","_, preds = torch.max(outputs, 1)\n","\n","plt.figure(figsize=(12, 6))\n","for i in range(8):\n","    plt.subplot(2, 4, i+1)\n","    ccnet_visuals(images[i].cpu(), title=f\"Pred: {preds[i].item()}, True: {labels[i].item()}\")\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"wW_84lL0aDxE"},"execution_count":null,"outputs":[]}]}